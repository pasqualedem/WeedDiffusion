{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a701f64",
   "metadata": {},
   "source": [
    "**DREAMBOOTH TRAINING**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e0f12",
   "metadata": {},
   "source": [
    "This notebook sets up and launches a **DreamBooth fine-tuning process** using Hugging Face’s official diffusers implementation (*train_dreambooth.py*).\n",
    "\n",
    "DreamBooth is a technique for fine-tuning text-to-image diffusion models (like Stable Diffusion) on a small set of images representing a specific subject or concept. Once trained, the model can generate novel images of that subject using a custom textual prompt.\n",
    "\n",
    "In this project, DreamBooth is used to teach the model how to generate more realistic samples of crops and weeds, using class-specific images and instance prompts.\n",
    "\n",
    "This notebook defines the necessary configuration and launches the training via the accelerate CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a5df0",
   "metadata": {},
   "source": [
    "This cell sets up the default configuration for Hugging Face’s accelerate library. It writes a basic config file needed to launch distributed training jobs efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7943d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7aec1",
   "metadata": {},
   "source": [
    "The following block launches the DreamBooth training process using the Stable Diffusion v1.5 model. The fine-tuning is performed on a small set of images, using a specific instance prompt. This prompt includes a unique identifier token —**\"sks\"** in this case— that acts as a placeholder for the learned concept. By training the model to associate *\"sks crops\"* (or sks weeds for weeds model) with the visual features of the target plant (e.g., sugar beet crop), we can later generate new variations by invoking that token in different prompts.\n",
    "\n",
    "Unlike general-purpose fine-tuning, this training is **intentionally overfitted** to a single class —sugar beet weed (*or crops if you change the instance dir*)— under very specific visual conditions (e.g., background soil, plant morphology). The goal is not to generalize to other plants, but to allow the model to **reproduce the same plant in varied configurations**, such as different leaf arrangements or positions in the field.\n",
    "\n",
    "To support this goal:\n",
    "- Class prior preservation (used to reduce overfitting by mixing in generic class images) has been disabled.\n",
    "- Class images —additional generic images of the broader category (e.g., other types of plants)— have also been excluded from training.\n",
    "- A relatively high number of training steps (4000) ensures that the model captures the fine visual details of the target without any abstraction.\n",
    "\n",
    "Key points:\n",
    "- Uses fp16 mixed precision for efficiency.\n",
    "- Trains both the UNet and text encoder.\n",
    "- Enables gradient checkpointing to save memory.\n",
    "- Periodic checkpoints every 1000 steps for traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "# instance_dir is the directory containing images of the subject to train on\n",
    "INSTANCE_DIR = \"subset_crops\"\n",
    "# output_dir is the directory where the trained model will be saved\n",
    "OUTPUT_DIR = \"model_crops\"\n",
    "\n",
    "command = [\n",
    "    \"accelerate\", \"launch\", \"train_dreambooth.py\",\n",
    "    \"--pretrained_model_name_or_path\", MODEL_NAME,\n",
    "    \"--instance_data_dir\", INSTANCE_DIR,\n",
    "    \"--output_dir\", OUTPUT_DIR,\n",
    "    \"--instance_prompt\", \"sks crop\",\n",
    "    \"--resolution\", \"512\",\n",
    "    \"--train_batch_size=1\",\n",
    "    \"--gradient_accumulation_steps=1\",\n",
    "    \"--learning_rate=1e-6\",\n",
    "    \"--lr_scheduler=constant\",\n",
    "    \"--lr_warmup_steps=0\",\n",
    "    \"--max_train_steps=4000\",\n",
    "    \"--train_text_encoder\",\n",
    "    \"--mixed_precision=fp16\",\n",
    "    \"--gradient_checkpointing\",\n",
    "    \"--checkpointing_steps=1000\"\n",
    "]\n",
    "\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3a5d1",
   "metadata": {},
   "source": [
    "After fine-tuning, by using the same custom token (**\"sks\"**) from training in the prompt, we can generates new images. This allows us to visually inspect how well the model has learned the appearance of the target plant class and how it can reproduce it in varied but consistent ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"models/model_crops\", \n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Prompt used in the training\n",
    "prompt = \"sks crop\"\n",
    "\n",
    "images = pipe(prompt=prompt, num_inference_steps=300, guidance_scale=7.5, num_images_per_prompt=1, height=1024, width=1024).images\n",
    "\n",
    "# Show all images\n",
    "for i, img in enumerate(images):\n",
    "    img.show(title=f\"Crop {i+1}\")\n",
    "\n",
    "# Save generated images\n",
    "for i, img in enumerate(images):\n",
    "    img.save(f\"generated_crop_{i+1}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreambooth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
